{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "data_dir = '/floyd/input/flower_data'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor()])\n",
    "#data_transforms = transforms.Compose([transforms.Resize(255),\n",
    "#                                transforms.CenterCrop(224),\n",
    "#                                transforms.ToTensor()])\n",
    "#dataset = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(data_dir + '/valid', transform=valid_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth\n",
      "100%|██████████| 553433881/553433881 [00:06<00:00, 90599770.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Loading network\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# move the model to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a classifier\n",
    "classifier = nn.Sequential(nn.Linear(7 * 7 * 512, 4096),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(0.2),\n",
    "                          nn.Linear(4096, 2024),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(0.2),\n",
    "                          nn.Linear(2024, 102),\n",
    "                          nn.LogSoftmax(dim=1))\n",
    "\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Specify Loss Function and Optimizer ***\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.002)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.062930 \tValidation Loss: 0.042601\n",
      "Validation loss decreased (inf --> 0.042601).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.033633 \tValidation Loss: 0.042101\n",
      "Validation loss decreased (0.042601 --> 0.042101).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.029775 \tValidation Loss: 0.032892\n",
      "Validation loss decreased (0.042101 --> 0.032892).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.028149 \tValidation Loss: 0.037028\n",
      "Epoch: 5 \tTraining Loss: 0.026540 \tValidation Loss: 0.032706\n",
      "Validation loss decreased (0.032892 --> 0.032706).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# *** Train the Network ***\n",
    "# number of epochs to train the model\n",
    "model.to(device)\n",
    "n_epochs = 5\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    # train the model #\n",
    "    model.train()\n",
    "    for data, target in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        logprobs = model.forward(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(logprobs, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "    # validate the model #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in validloader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            logprobs = model.forward(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(logprobs, target)\n",
    "            # update average validation loss\n",
    "            valid_loss += loss.item()\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    valid_loss = valid_loss/len(validloader.dataset)\n",
    "    # print training/validation statistics\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'models/model_flowers.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "# // *** Train the Network ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'epoch':n_epochs,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict':optimizer.state_dict(),\n",
    "               'loss':loss}\n",
    "\n",
    "torch.save(checkpoint, 'models/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    # Freeze parameters so we don't backprop through them\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    #defining a classifier\n",
    "    classifier = nn.Sequential(nn.Linear(7 * 7 * 512, 4096),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout(0.2),\n",
    "                              nn.Linear(4096, 2024),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout(0.2),\n",
    "                              nn.Linear(2024, 102),\n",
    "                              nn.LogSoftmax(dim=1))\n",
    "    model.classifier = classifier\n",
    "    return model\n",
    "\n",
    "def load_checkpoint(filepath):\n",
    "    model = prepare_model()\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.002)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    return model\n",
    "\n",
    "model_new = load_checkpoint('models/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    image_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "    return image_transforms(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.9999943, 6.1996348e-06, 6.723769e-08, 2.5966582e-08, 2.6282478e-09], ['56', '59', '46', '58', '100'])\n",
      "Actual class: bishop of llandaff\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = os.listdir(dir)\n",
    "    classes.sort()\n",
    "    class_to_idx = {i: classes[i] for i in range(len(classes))}\n",
    "    return class_to_idx\n",
    "\n",
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    pil_img = Image.open(img_path)\n",
    "    image = process_image(pil_img)\n",
    "    image.unsqueeze_(0)\n",
    "    model.eval()\n",
    "    logps = model.forward(image)\n",
    "    ps = torch.exp(logps)\n",
    "    top_p, top_class = ps.topk(5, dim=1)\n",
    "    top_p = np.squeeze(top_p.detach().numpy())\n",
    "    top_p = [val for val in top_p]\n",
    "    top_class = np.squeeze(top_class.detach().numpy())\n",
    "    idx_to_classes = find_classes(valid_dir)\n",
    "    top_class_keys = [idx_to_classes[value] for value in top_class]\n",
    "    return top_p, top_class_keys \n",
    "\n",
    "img_path = valid_dir + '/56/image_02771.jpg'\n",
    "print(predict(img_path, model_new))\n",
    "actual_class = cat_to_name[img_path.split('/')[5]]\n",
    "print(\"Actual class: {}\".format(actual_class))\n",
    "#labels = {int(key):value for (key, value) in cat_to_name}\n",
    "#os.listdir(valid_dir)\n",
    "#classes = [d.name for d in os.scandir(valid_dir) if d.is_dir()]\n",
    "#print(classes)\n",
    "#imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
